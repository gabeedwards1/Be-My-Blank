# pip install pandas requests beautifulsoup4 python-dateutil
import json
import re
from dataclasses import dataclass
from typing import List, Dict
import requests
import pandas as pd
from bs4 import BeautifulSoup
from dateutil import parser as dtp

FULL_YEAR_URL = "https://www.timeanddate.com/holidays/fun/"  # full-year index

@dataclass
class FunHoliday:
    date_iso: str   # YYYY-MM-DD
    weekday: str
    name: str

def fetch_html(url: str) -> str:
    r = requests.get(url, timeout=30, headers={"User-Agent": "holiday-buddy/1.0"})
    r.raise_for_status()
    return r.text

def extract_year_from_heading(html: str) -> int:
    # e.g., "Fun Holidays — Full Year 2025"
    m = re.search(r"Full Year\s+(\d{4})", html)
    if not m:
        # monthly pages show "January 2026", etc.—fallback to first 4-digit year on page
        m = re.search(r"\b(20\d{2})\b", html)
    if not m:
        raise ValueError("Could not detect year on page.")
    return int(m.group(1))

def parse_with_pandas(html: str, year: int) -> List[FunHoliday]:
    # read all tables; find one that has columns for Date/Weekday/Holiday Name
    tables = pd.read_html(html)
    target = None
    for df in tables:
        cols = [str(c).strip().lower() for c in df.columns]
        if len(cols) >= 3 and "date" in cols[0] and "weekday" in cols[1]:
            # last column should be the holiday name
            target = df
            break
    if target is None:
        raise ValueError("Full-year table not found via pandas.")
    # Normalize columns
    target.columns = [str(c).strip() for c in target.columns]
    rows = []
    for _, row in target.iterrows():
        month_day = str(row[target.columns[0]]).strip()     # e.g., "Jan 2"
        weekday   = str(row[target.columns[1]]).strip()     # e.g., "Thursday"
        name      = str(row[target.columns[-1]]).strip()    # holiday name
        if not month_day or not name or month_day.lower().startswith("date"):
            continue
        # Build ISO date using detected year
        dt = dtp.parse(f"{month_day} {year}")
        rows.append(FunHoliday(date_iso=dt.strftime("%Y-%m-%d"), weekday=weekday, name=name))
    return rows

def parse_with_bs4(html: str, year: int) -> List[FunHoliday]:
    # Fallback parser: walk the section after the "Full Year" heading and read triplets
    soup = BeautifulSoup(html, "html.parser")
    # Find the heading that contains "Fun Holidays — Full Year"
    heading = None
    for h in soup.find_all(["h2","h3"]):
        if h.get_text(" ", strip=True).lower().find("full year") != -1:
            heading = h
            break
    if not heading:
        raise ValueError("Could not locate full-year section.")

    # Find the first table after the heading
    table = heading.find_next("table")
    if not table:
        raise ValueError("No table found after full-year heading.")

    rows = []
    for tr in table.find_all("tr"):
        tds = [td.get_text(" ", strip=True) for td in tr.find_all(["td","th"])]
        if len(tds) < 3:
            continue
        if tds[0].lower() == "date":
            continue
        month_day, weekday = tds[0], tds[1]
        # Holiday name may be in a link in the last column
        last_td = tr.find_all("td")[-1] if tr.find_all("td") else None
        if not last_td:
            continue
        name = last_td.get_text(" ", strip=True)
        if not name:
            continue
        dt = dtp.parse(f"{month_day} {year}")
        rows.append(FunHoliday(date_iso=dt.strftime("%Y-%m-%d"), weekday=weekday, name=name))
    return rows

def dedupe(rows: List[FunHoliday]) -> List[FunHoliday]:
    seen = set()
    out = []
    for r in rows:
        key = (r.date_iso, r.name)
        if key in seen:
            continue
        seen.add(key)
        out.append(r)
    return out

def save_json(rows: List[FunHoliday]):
    # 1) list form
    list_payload = [
        {"date": r.date_iso, "weekday": r.weekday, "name": r.name, "source": "timeanddate.com"}
        for r in rows
    ]
    with open("fun_holidays_list.json", "w", encoding="utf-8") as f:
        json.dump(list_payload, f, ensure_ascii=False, indent=2)

    # 2) map by date -> [names]
    by_date: Dict[str, List[str]] = {}
    for r in rows:
        by_date.setdefault(r.date_iso, []).append(r.name)
    with open("fun_holidays_by_date.json", "w", encoding="utf-8") as f:
        json.dump(by_date, f, ensure_ascii=False, indent=2)

def main():
    html = fetch_html(FULL_YEAR_URL)
    year = extract_year_from_heading(html)
    try:
        rows = parse_with_pandas(html, year)
    except Exception:
        rows = parse_with_bs4(html, year)
    rows = dedupe(rows)
    save_json(rows)
    print(f"Saved {len(rows)} fun holidays for {year} to:")
    print(" - fun_holidays_list.json")
    print(" - fun_holidays_by_date.json")

if __name__ == "__main__":
    main()
